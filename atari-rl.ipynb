{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farahbakhsh3/X1/blob/master/atari-rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-gmiYaX8U0df",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5868
        },
        "outputId": "c918b37b-f9f1-41a3-9130-d1558f959f64"
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/rbgg6mbunq8qtr3/newCallback.py\n",
        "#!wget ????\n",
        "!wget https://www.dropbox.com/s/g69yj8ia900bk1p/dqn_Atari_512.py\n",
        "\n",
        "#!pip install tensorflow==1.6.0\n",
        "!apt-get install cmake\n",
        "#!apt-get install swig\n",
        "!pip install gym[atari]\n",
        "#!pip install gym[Box2D]\n",
        "#!pip install keras-rl\n",
        "\n",
        "!pip install dropbox\n",
        "\n",
        "!git clone https://github.com/matthiasplappert/keras-rl.git\n",
        "#import os\n",
        "#os.chdir('keras-rl/')\n",
        "%cd keras-rl/\n",
        "!python setup.py install\n",
        "#!pip install -e .\n",
        "#os.chdir('../')\n",
        "import rl\n",
        "%cd ../\n",
        "\n",
        "\n",
        "!python3 ./dqn_Atari_512.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-03 07:50:16--  https://www.dropbox.com/s/rbgg6mbunq8qtr3/newCallback.py\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/rbgg6mbunq8qtr3/newCallback.py [following]\n",
            "--2019-02-03 07:50:16--  https://www.dropbox.com/s/raw/rbgg6mbunq8qtr3/newCallback.py\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf28e9a5d25accd7d6825659c78.dl.dropboxusercontent.com/cd/0/inline/AalVIS5P6E5wWA-2ZFWZ6kIoMmIW2EkWS2wBTMvNsf_cIJg6-E2Ey5BSlOQivLpTzrZALUJ0yek-mCdhVPaZdItTMvj8375b8makAM22snkIXQ/file# [following]\n",
            "--2019-02-03 07:50:17--  https://ucf28e9a5d25accd7d6825659c78.dl.dropboxusercontent.com/cd/0/inline/AalVIS5P6E5wWA-2ZFWZ6kIoMmIW2EkWS2wBTMvNsf_cIJg6-E2Ey5BSlOQivLpTzrZALUJ0yek-mCdhVPaZdItTMvj8375b8makAM22snkIXQ/file\n",
            "Resolving ucf28e9a5d25accd7d6825659c78.dl.dropboxusercontent.com (ucf28e9a5d25accd7d6825659c78.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to ucf28e9a5d25accd7d6825659c78.dl.dropboxusercontent.com (ucf28e9a5d25accd7d6825659c78.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17229 (17K) [text/plain]\n",
            "Saving to: ‘newCallback.py’\n",
            "\n",
            "newCallback.py      100%[===================>]  16.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-02-03 07:50:17 (200 MB/s) - ‘newCallback.py’ saved [17229/17229]\n",
            "\n",
            "--2019-02-03 07:50:18--  https://www.dropbox.com/s/g69yj8ia900bk1p/dqn_Atari_512.py\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/g69yj8ia900bk1p/dqn_Atari_512.py [following]\n",
            "--2019-02-03 07:50:18--  https://www.dropbox.com/s/raw/g69yj8ia900bk1p/dqn_Atari_512.py\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc4866e86d3f4880ab3ab4dff20c.dl.dropboxusercontent.com/cd/0/inline/AanC4ZyCLbYDSi4AqfR9CdubNa2Ou21OITpwxLgeUvFcNcr9mxH0S6Bmf7CL8kVbl3U0kpFGoA0lDzPln9xFIYH9as1ZzyyJ5mtYBtzuiU9qHQ/file# [following]\n",
            "--2019-02-03 07:50:18--  https://uc4866e86d3f4880ab3ab4dff20c.dl.dropboxusercontent.com/cd/0/inline/AanC4ZyCLbYDSi4AqfR9CdubNa2Ou21OITpwxLgeUvFcNcr9mxH0S6Bmf7CL8kVbl3U0kpFGoA0lDzPln9xFIYH9as1ZzyyJ5mtYBtzuiU9qHQ/file\n",
            "Resolving uc4866e86d3f4880ab3ab4dff20c.dl.dropboxusercontent.com (uc4866e86d3f4880ab3ab4dff20c.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc4866e86d3f4880ab3ab4dff20c.dl.dropboxusercontent.com (uc4866e86d3f4880ab3ab4dff20c.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3966 (3.9K) [text/plain]\n",
            "Saving to: ‘dqn_Atari_512.py’\n",
            "\n",
            "dqn_Atari_512.py    100%[===================>]   3.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-02-03 07:50:19 (464 MB/s) - ‘dqn_Atari_512.py’ saved [3966/3966]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: atari_py>=0.1.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.0.0)\n",
            "Requirement already satisfied: PyOpenGL in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->gym[atari]) (0.46)\n",
            "Collecting dropbox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/20/4e55247e4e474aa741d209a533f4e490ea08a0f2cf84e7fa49cd38c3b54b/dropbox-9.3.0-py3-none-any.whl (503kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from dropbox) (1.11.0)\n",
            "Requirement already satisfied: requests>=2.16.2 in /usr/local/lib/python3.6/dist-packages (from dropbox) (2.18.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.16.2->dropbox) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.16.2->dropbox) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.16.2->dropbox) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.16.2->dropbox) (2018.11.29)\n",
            "Installing collected packages: dropbox\n",
            "Successfully installed dropbox-9.3.0\n",
            "Cloning into 'keras-rl'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 1682 (delta 11), reused 19 (delta 8), pack-reused 1649\u001b[K\n",
            "Receiving objects: 100% (1682/1682), 1.38 MiB | 12.27 MiB/s, done.\n",
            "Resolving deltas: 100% (1039/1039), done.\n",
            "/content/keras-rl\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating keras_rl.egg-info\n",
            "writing keras_rl.egg-info/PKG-INFO\n",
            "writing dependency_links to keras_rl.egg-info/dependency_links.txt\n",
            "writing requirements to keras_rl.egg-info/requires.txt\n",
            "writing top-level names to keras_rl.egg-info/top_level.txt\n",
            "writing manifest file 'keras_rl.egg-info/SOURCES.txt'\n",
            "writing manifest file 'keras_rl.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/rl\n",
            "copying rl/callbacks.py -> build/lib/rl\n",
            "copying rl/policy.py -> build/lib/rl\n",
            "copying rl/core.py -> build/lib/rl\n",
            "copying rl/processors.py -> build/lib/rl\n",
            "copying rl/__init__.py -> build/lib/rl\n",
            "copying rl/memory.py -> build/lib/rl\n",
            "copying rl/random.py -> build/lib/rl\n",
            "copying rl/util.py -> build/lib/rl\n",
            "creating build/lib/utils\n",
            "copying utils/__init__.py -> build/lib/utils\n",
            "creating build/lib/tests\n",
            "copying tests/__init__.py -> build/lib/tests\n",
            "creating build/lib/rl/common\n",
            "copying rl/common/cmd_util.py -> build/lib/rl/common\n",
            "copying rl/common/misc_util.py -> build/lib/rl/common\n",
            "copying rl/common/__init__.py -> build/lib/rl/common\n",
            "copying rl/common/tile_images.py -> build/lib/rl/common\n",
            "creating build/lib/rl/agents\n",
            "copying rl/agents/sarsa.py -> build/lib/rl/agents\n",
            "copying rl/agents/ddpg.py -> build/lib/rl/agents\n",
            "copying rl/agents/__init__.py -> build/lib/rl/agents\n",
            "copying rl/agents/cem.py -> build/lib/rl/agents\n",
            "copying rl/agents/dqn.py -> build/lib/rl/agents\n",
            "creating build/lib/rl/common/vec_env\n",
            "copying rl/common/vec_env/subproc_env_vec.py -> build/lib/rl/common/vec_env\n",
            "copying rl/common/vec_env/__init__.py -> build/lib/rl/common/vec_env\n",
            "creating build/lib/utils/gym\n",
            "copying utils/gym/__init__.py -> build/lib/utils/gym\n",
            "copying utils/gym/prng.py -> build/lib/utils/gym\n",
            "creating build/lib/utils/gym/spaces\n",
            "copying utils/gym/spaces/__init__.py -> build/lib/utils/gym/spaces\n",
            "copying utils/gym/spaces/discrete.py -> build/lib/utils/gym/spaces\n",
            "creating build/lib/utils/gym/envs\n",
            "copying utils/gym/envs/__init__.py -> build/lib/utils/gym/envs\n",
            "copying utils/gym/envs/twoRoundDeterministicRewardEnv.py -> build/lib/utils/gym/envs\n",
            "creating build/lib/tests/rl\n",
            "copying tests/rl/test_util.py -> build/lib/tests/rl\n",
            "copying tests/rl/__init__.py -> build/lib/tests/rl\n",
            "copying tests/rl/test_memory.py -> build/lib/tests/rl\n",
            "copying tests/rl/util.py -> build/lib/tests/rl\n",
            "copying tests/rl/test_core.py -> build/lib/tests/rl\n",
            "creating build/lib/tests/rl/agents\n",
            "copying tests/rl/agents/test_dqn.py -> build/lib/tests/rl/agents\n",
            "copying tests/rl/agents/test_ddpg.py -> build/lib/tests/rl/agents\n",
            "copying tests/rl/agents/__init__.py -> build/lib/tests/rl/agents\n",
            "copying tests/rl/agents/test_cem.py -> build/lib/tests/rl/agents\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/rl\n",
            "creating build/bdist.linux-x86_64/egg/rl/common\n",
            "copying build/lib/rl/common/cmd_util.py -> build/bdist.linux-x86_64/egg/rl/common\n",
            "copying build/lib/rl/common/misc_util.py -> build/bdist.linux-x86_64/egg/rl/common\n",
            "copying build/lib/rl/common/__init__.py -> build/bdist.linux-x86_64/egg/rl/common\n",
            "creating build/bdist.linux-x86_64/egg/rl/common/vec_env\n",
            "copying build/lib/rl/common/vec_env/subproc_env_vec.py -> build/bdist.linux-x86_64/egg/rl/common/vec_env\n",
            "copying build/lib/rl/common/vec_env/__init__.py -> build/bdist.linux-x86_64/egg/rl/common/vec_env\n",
            "copying build/lib/rl/common/tile_images.py -> build/bdist.linux-x86_64/egg/rl/common\n",
            "copying build/lib/rl/callbacks.py -> build/bdist.linux-x86_64/egg/rl\n",
            "creating build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/sarsa.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/ddpg.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/__init__.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/cem.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/dqn.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/policy.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/core.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/processors.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/__init__.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/memory.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/random.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/util.py -> build/bdist.linux-x86_64/egg/rl\n",
            "creating build/bdist.linux-x86_64/egg/utils\n",
            "copying build/lib/utils/__init__.py -> build/bdist.linux-x86_64/egg/utils\n",
            "creating build/bdist.linux-x86_64/egg/utils/gym\n",
            "copying build/lib/utils/gym/__init__.py -> build/bdist.linux-x86_64/egg/utils/gym\n",
            "creating build/bdist.linux-x86_64/egg/utils/gym/spaces\n",
            "copying build/lib/utils/gym/spaces/__init__.py -> build/bdist.linux-x86_64/egg/utils/gym/spaces\n",
            "copying build/lib/utils/gym/spaces/discrete.py -> build/bdist.linux-x86_64/egg/utils/gym/spaces\n",
            "creating build/bdist.linux-x86_64/egg/utils/gym/envs\n",
            "copying build/lib/utils/gym/envs/__init__.py -> build/bdist.linux-x86_64/egg/utils/gym/envs\n",
            "copying build/lib/utils/gym/envs/twoRoundDeterministicRewardEnv.py -> build/bdist.linux-x86_64/egg/utils/gym/envs\n",
            "copying build/lib/utils/gym/prng.py -> build/bdist.linux-x86_64/egg/utils/gym\n",
            "creating build/bdist.linux-x86_64/egg/tests\n",
            "creating build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/rl/test_util.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "creating build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/agents/test_dqn.py -> build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/agents/test_ddpg.py -> build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/agents/__init__.py -> build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/agents/test_cem.py -> build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/__init__.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/rl/test_memory.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/rl/util.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/rl/test_core.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/__init__.py -> build/bdist.linux-x86_64/egg/tests\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/cmd_util.py to cmd_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/misc_util.py to misc_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/vec_env/subproc_env_vec.py to subproc_env_vec.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/vec_env/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/tile_images.py to tile_images.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/callbacks.py to callbacks.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/sarsa.py to sarsa.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/ddpg.py to ddpg.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/cem.py to cem.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/dqn.py to dqn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/policy.py to policy.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/core.py to core.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/processors.py to processors.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/memory.py to memory.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/random.py to random.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/util.py to util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/spaces/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/spaces/discrete.py to discrete.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/envs/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/envs/twoRoundDeterministicRewardEnv.py to twoRoundDeterministicRewardEnv.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/prng.py to prng.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/test_util.py to test_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/agents/test_dqn.py to test_dqn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/agents/test_ddpg.py to test_ddpg.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/agents/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/agents/test_cem.py to test_cem.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/test_memory.py to test_memory.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/util.py to util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/test_core.py to test_core.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "tests.rl.__pycache__.test_core.cpython-36: module references __file__\n",
            "tests.rl.__pycache__.test_memory.cpython-36: module references __file__\n",
            "tests.rl.__pycache__.test_util.cpython-36: module references __file__\n",
            "tests.rl.agents.__pycache__.test_dqn.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/keras_rl-0.4.2-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing keras_rl-0.4.2-py3.6.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/keras_rl-0.4.2-py3.6.egg\n",
            "Extracting keras_rl-0.4.2-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding keras-rl 0.4.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/keras_rl-0.4.2-py3.6.egg\n",
            "Processing dependencies for keras-rl==0.4.2\n",
            "Searching for Keras==2.2.4\n",
            "Best match: Keras 2.2.4\n",
            "Adding Keras 2.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.1.0\n",
            "Best match: scipy 1.1.0\n",
            "Adding scipy 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==3.13\n",
            "Best match: PyYAML 3.13\n",
            "Adding PyYAML 3.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Preprocessing==1.0.8\n",
            "Best match: Keras-Preprocessing 1.0.8\n",
            "Adding Keras-Preprocessing 1.0.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.14.6\n",
            "Best match: numpy 1.14.6\n",
            "Adding numpy 1.14.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Applications==1.0.7\n",
            "Best match: Keras-Applications 1.0.7\n",
            "Adding Keras-Applications 1.0.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.8.0\n",
            "Best match: h5py 2.8.0\n",
            "Adding h5py 2.8.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.11.0\n",
            "Best match: six 1.11.0\n",
            "Adding six 1.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for keras-rl==0.4.2\n",
            "/content\n",
            "Using TensorFlow backend.\n",
            "========================\n",
            "train\n",
            "BoxingDeterministic-v4\n",
            "None\n",
            "========================\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 18)                9234      \n",
            "=================================================================\n",
            "Total params: 1,693,362\n",
            "Trainable params: 1,693,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "2019-02-03 07:50:37.820230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-02-03 07:50:37.820629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-02-03 07:50:37.820678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-02-03 07:50:38.708862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-02-03 07:50:38.708922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-02-03 07:50:38.708952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-02-03 07:50:38.709181: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-02-03 07:50:38.709278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Traceback (most recent call last):\n",
            "  File \"./dqn_Atari_512.py\", line 98, in <module>\n",
            "    dqn.load_weights(weights_filename) \n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_rl-0.4.2-py3.6.egg/rl/agents/dqn.py\", line 209, in load_weights\n",
            "    self.model.load_weights(filepath)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\", line 1157, in load_weights\n",
            "    with h5py.File(filepath, mode='r') as f:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 312, in __init__\n",
            "    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 142, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 78, in h5py.h5f.open\n",
            "OSError: Unable to open file (unable to open file: name = 'dqn512_BoxingDeterministic-v4_weights.h5f', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RWnF2jITVOki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2818
        },
        "outputId": "48a13081-9652-4129-e50e-1d9e5c2e16a5"
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "   \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, Permute\n",
        "from keras.layers import MaxPooling2D, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "   \n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory, EpisodeParameterMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger#, ModelIntervalCheckpoint\n",
        "from newCallback import ModelIntervalCheckpoint\n",
        "   \n",
        "   \n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "   \n",
        "   \n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L') \n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')  \n",
        "   \n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "   \n",
        "    # def process_reward(self, reward):\n",
        "    #     return np.clip(reward, -1., 1.)\n",
        "    #     return reward\n",
        "   \n",
        "    # def process_action(self, action):        \n",
        "    #     return action\n",
        "   \n",
        "   \n",
        "class args():\n",
        "    pass\n",
        "   \n",
        "   \n",
        "args.mode = 'train'\n",
        "args.env_name = 'Riverraid-v4'\n",
        "args.weights = None\n",
        "   \n",
        "print('========================')\n",
        "print(args.mode)\n",
        "print(args.env_name)\n",
        "print(args.weights)\n",
        "print('========================')\n",
        "   \n",
        "   \n",
        "env = gym.make(args.env_name)\n",
        "np.random.seed(7)\n",
        "env.seed(7)\n",
        "nb_actions = env.action_space.n\n",
        "   \n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "# (width, height, channels)\n",
        "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "model.add(Conv2D(8, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(16, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=(2, 2), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "print(model.summary())\n",
        "   \n",
        "# memory = EpisodeParameterMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "memory = SequentialMemory(limit=10000000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()\n",
        "   \n",
        "# policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
        "#                              nb_steps=1000000)\n",
        "policy = BoltzmannQPolicy()\n",
        "# policy = EpsGreedyQPolicy(eps=.1)\n",
        "   \n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
        "               processor=processor, nb_steps_warmup=1000, gamma=.99, target_model_update=1000,\n",
        "               train_interval=1, delta_clip=1.)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "   \n",
        "weights_filename = 'dqn512_{}_weights.h5f'.format(args.env_name)\n",
        "checkpoint_weights_filename = 'dqn512_' + args.env_name + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn512_{}_log.json'.format(args.env_name)\n",
        "if args.mode == 'train':\n",
        "       \n",
        "    callbacks = None\n",
        "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=50001, verbose=1, updropbox=True)]\n",
        "    # callbacks += [FileLogger(log_filename, interval=100)]\n",
        "    \n",
        "    if (args.weights):\n",
        "      dqn.load_weights(weights_filename) \n",
        "      print('Weights Loaded: ', weights_filename)\n",
        "      \n",
        "    dqn.fit(env, callbacks=callbacks, nb_steps=1000000, verbose=2, log_interval=100000, visualize=False)\n",
        "   \n",
        "    dqn.save_weights(weights_filename, overwrite=True)\n",
        "    print('Weights Saved: ', weights_filename)\n",
        "   \n",
        "    dqn.test(env, nb_episodes=10, visualize=False)\n",
        "elif args.mode == 'test':\n",
        "    if args.weights:\n",
        "        weights_filename = args.weights\n",
        "    dqn.load_weights(weights_filename)\n",
        "    print('Weights Loaded: ', weights_filename)\n",
        "    dqn.test(env, nb_episodes=10, visualize=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================\n",
            "train\n",
            "Boxing-v4\n",
            "None\n",
            "========================\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_4 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 84, 84, 32)        1184      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 42, 42, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 42, 42, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 42, 42, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 21, 21, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 21, 21, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 21, 21, 128)       32896     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 10, 10, 128)       512       \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               6554112   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 18)                9234      \n",
            "=================================================================\n",
            "Total params: 6,616,818\n",
            "Trainable params: 6,616,370\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 1000000 steps ...\n",
            "   2366/1000000: episode: 1, duration: 73.510s, episode steps: 2366, steps per second: 32, episode reward: 3.000, mean reward: 0.001 [-2.000, 2.000], mean action: 8.384 [0.000, 17.000], mean observation: 124.843 [0.000, 214.000], loss: 0.085411, mean_absolute_error: 0.301851, mean_q: 0.412486\n",
            "   4757/1000000: episode: 2, duration: 116.989s, episode steps: 2391, steps per second: 20, episode reward: -8.000, mean reward: -0.003 [-2.000, 2.000], mean action: 8.530 [0.000, 17.000], mean observation: 124.817 [0.000, 214.000], loss: 0.396469, mean_absolute_error: 1.202429, mean_q: 1.629011\n",
            "   7112/1000000: episode: 3, duration: 116.099s, episode steps: 2355, steps per second: 20, episode reward: -5.000, mean reward: -0.002 [-2.000, 2.000], mean action: 9.553 [0.000, 17.000], mean observation: 124.844 [0.000, 214.000], loss: 0.097150, mean_absolute_error: 2.078681, mean_q: 2.477632\n",
            "   9485/1000000: episode: 4, duration: 116.565s, episode steps: 2373, steps per second: 20, episode reward: 3.000, mean reward: 0.001 [-2.000, 2.000], mean action: 8.456 [0.000, 17.000], mean observation: 124.843 [0.000, 214.000], loss: 0.128177, mean_absolute_error: 2.411652, mean_q: 2.625733\n",
            "  11857/1000000: episode: 5, duration: 116.339s, episode steps: 2372, steps per second: 20, episode reward: -4.000, mean reward: -0.002 [-2.000, 2.000], mean action: 8.607 [0.000, 17.000], mean observation: 124.860 [0.000, 214.000], loss: 0.202103, mean_absolute_error: 3.178647, mean_q: 3.520030\n",
            "  14253/1000000: episode: 6, duration: 117.940s, episode steps: 2396, steps per second: 20, episode reward: -1.000, mean reward: -0.000 [-2.000, 2.000], mean action: 8.975 [0.000, 17.000], mean observation: 124.832 [0.000, 214.000], loss: 0.268535, mean_absolute_error: 4.122347, mean_q: 4.566248\n",
            "  16637/1000000: episode: 7, duration: 117.362s, episode steps: 2384, steps per second: 20, episode reward: -20.000, mean reward: -0.008 [-2.000, 2.000], mean action: 8.359 [0.000, 17.000], mean observation: 124.866 [0.000, 214.000], loss: 10.205175, mean_absolute_error: 47.055511, mean_q: 58.419640\n",
            "  19019/1000000: episode: 8, duration: 118.671s, episode steps: 2382, steps per second: 20, episode reward: -27.000, mean reward: -0.011 [-2.000, 2.000], mean action: 7.924 [0.000, 17.000], mean observation: 124.839 [0.000, 214.000], loss: 23.874853, mean_absolute_error: 195.024689, mean_q: 221.595200\n",
            "  21402/1000000: episode: 9, duration: 117.157s, episode steps: 2383, steps per second: 20, episode reward: -29.000, mean reward: -0.012 [-2.000, 2.000], mean action: 7.151 [0.000, 17.000], mean observation: 124.839 [0.000, 214.000], loss: 26.938431, mean_absolute_error: 233.082092, mean_q: 261.166687\n",
            "  23790/1000000: episode: 10, duration: 117.223s, episode steps: 2388, steps per second: 20, episode reward: -21.000, mean reward: -0.009 [-2.000, 2.000], mean action: 9.346 [0.000, 17.000], mean observation: 124.839 [0.000, 214.000], loss: 34.333492, mean_absolute_error: 303.243652, mean_q: 338.563324\n",
            "  26170/1000000: episode: 11, duration: 117.853s, episode steps: 2380, steps per second: 20, episode reward: -26.000, mean reward: -0.011 [-2.000, 2.000], mean action: 9.457 [0.000, 17.000], mean observation: 124.851 [0.000, 214.000], loss: 34.747478, mean_absolute_error: 317.380554, mean_q: 352.429291\n",
            "  28542/1000000: episode: 12, duration: 117.329s, episode steps: 2372, steps per second: 20, episode reward: -22.000, mean reward: -0.009 [-2.000, 2.000], mean action: 8.241 [0.000, 17.000], mean observation: 124.859 [0.000, 214.000], loss: 38.166374, mean_absolute_error: 353.291962, mean_q: 392.890747\n",
            "  30911/1000000: episode: 13, duration: 117.712s, episode steps: 2369, steps per second: 20, episode reward: -40.000, mean reward: -0.017 [-2.000, 2.000], mean action: 8.123 [0.000, 17.000], mean observation: 124.790 [0.000, 214.000], loss: 35.237282, mean_absolute_error: 344.063049, mean_q: 380.073303\n",
            "  33274/1000000: episode: 14, duration: 117.408s, episode steps: 2363, steps per second: 20, episode reward: -18.000, mean reward: -0.008 [-2.000, 2.000], mean action: 7.861 [0.000, 17.000], mean observation: 124.853 [0.000, 214.000], loss: 34.386623, mean_absolute_error: 340.299744, mean_q: 375.806000\n",
            "  35655/1000000: episode: 15, duration: 117.766s, episode steps: 2381, steps per second: 20, episode reward: -38.000, mean reward: -0.016 [-2.000, 2.000], mean action: 8.470 [0.000, 17.000], mean observation: 124.824 [0.000, 214.000], loss: 33.771454, mean_absolute_error: 333.356842, mean_q: 366.825745\n",
            "  38051/1000000: episode: 16, duration: 119.489s, episode steps: 2396, steps per second: 20, episode reward: -31.000, mean reward: -0.013 [-2.000, 2.000], mean action: 8.594 [0.000, 17.000], mean observation: 124.870 [0.000, 214.000], loss: 32.664513, mean_absolute_error: 325.947205, mean_q: 358.369598\n",
            "  40402/1000000: episode: 17, duration: 116.956s, episode steps: 2351, steps per second: 20, episode reward: -45.000, mean reward: -0.019 [-2.000, 2.000], mean action: 9.287 [0.000, 17.000], mean observation: 124.799 [0.000, 214.000], loss: 31.320747, mean_absolute_error: 313.085266, mean_q: 344.201385\n",
            "  42775/1000000: episode: 18, duration: 118.553s, episode steps: 2373, steps per second: 20, episode reward: -26.000, mean reward: -0.011 [-2.000, 2.000], mean action: 9.265 [0.000, 17.000], mean observation: 124.860 [0.000, 214.000], loss: 32.699173, mean_absolute_error: 327.191711, mean_q: 359.667114\n",
            "  45148/1000000: episode: 19, duration: 118.758s, episode steps: 2373, steps per second: 20, episode reward: -29.000, mean reward: -0.012 [-2.000, 2.000], mean action: 8.225 [0.000, 17.000], mean observation: 124.838 [0.000, 214.000], loss: 31.026461, mean_absolute_error: 312.277008, mean_q: 342.629944\n",
            "  47523/1000000: episode: 20, duration: 118.955s, episode steps: 2375, steps per second: 20, episode reward: -16.000, mean reward: -0.007 [-2.000, 2.000], mean action: 8.307 [0.000, 17.000], mean observation: 124.832 [0.000, 214.000], loss: 29.679060, mean_absolute_error: 300.633484, mean_q: 329.604340\n",
            "  49896/1000000: episode: 21, duration: 116.932s, episode steps: 2373, steps per second: 20, episode reward: -30.000, mean reward: -0.013 [-2.000, 2.000], mean action: 8.793 [0.000, 17.000], mean observation: 124.843 [0.000, 214.000], loss: 28.420189, mean_absolute_error: 289.636047, mean_q: 317.704407\n",
            "Step 50001: saving model to dqn512_Boxing-v4_weights_50001.h5f\n",
            "Step 50001: saving model to Dropbox: dqn512_Boxing-v4_weights_50001.h5f\n",
            "  52271/1000000: episode: 22, duration: 119.270s, episode steps: 2375, steps per second: 20, episode reward: -6.000, mean reward: -0.003 [-2.000, 2.000], mean action: 8.129 [0.000, 17.000], mean observation: 124.854 [0.000, 214.000], loss: 30.166624, mean_absolute_error: 305.488800, mean_q: 335.178040\n",
            "  54656/1000000: episode: 23, duration: 116.191s, episode steps: 2385, steps per second: 21, episode reward: -13.000, mean reward: -0.005 [-2.000, 2.000], mean action: 8.442 [0.000, 17.000], mean observation: 124.845 [0.000, 214.000], loss: 29.520086, mean_absolute_error: 301.184937, mean_q: 329.958618\n",
            "  57034/1000000: episode: 24, duration: 116.480s, episode steps: 2378, steps per second: 20, episode reward: -18.000, mean reward: -0.008 [-2.000, 2.000], mean action: 8.651 [0.000, 17.000], mean observation: 124.860 [0.000, 214.000], loss: 30.080608, mean_absolute_error: 307.053619, mean_q: 336.303009\n",
            "  59395/1000000: episode: 25, duration: 114.979s, episode steps: 2361, steps per second: 21, episode reward: -26.000, mean reward: -0.011 [-2.000, 2.000], mean action: 8.989 [0.000, 17.000], mean observation: 124.845 [0.000, 214.000], loss: 29.867331, mean_absolute_error: 305.346741, mean_q: 333.766937\n",
            "  61774/1000000: episode: 26, duration: 116.003s, episode steps: 2379, steps per second: 21, episode reward: -9.000, mean reward: -0.004 [-2.000, 2.000], mean action: 9.146 [0.000, 17.000], mean observation: 124.876 [0.000, 214.000], loss: 29.363329, mean_absolute_error: 300.210907, mean_q: 328.330353\n",
            "  64158/1000000: episode: 27, duration: 116.490s, episode steps: 2384, steps per second: 20, episode reward: -42.000, mean reward: -0.018 [-2.000, 2.000], mean action: 8.867 [0.000, 17.000], mean observation: 124.872 [0.000, 214.000], loss: 30.817154, mean_absolute_error: 316.936035, mean_q: 347.027069\n",
            "  66558/1000000: episode: 28, duration: 116.751s, episode steps: 2400, steps per second: 21, episode reward: -10.000, mean reward: -0.004 [-2.000, 2.000], mean action: 8.579 [0.000, 17.000], mean observation: 124.840 [0.000, 214.000], loss: 31.955904, mean_absolute_error: 331.775909, mean_q: 362.696838\n",
            "  68934/1000000: episode: 29, duration: 116.237s, episode steps: 2376, steps per second: 20, episode reward: -36.000, mean reward: -0.015 [-2.000, 2.000], mean action: 8.375 [0.000, 17.000], mean observation: 124.868 [0.000, 214.000], loss: 33.306847, mean_absolute_error: 346.728577, mean_q: 378.812347\n",
            "  71321/1000000: episode: 30, duration: 116.572s, episode steps: 2387, steps per second: 20, episode reward: -34.000, mean reward: -0.014 [-2.000, 2.000], mean action: 9.208 [0.000, 17.000], mean observation: 124.843 [0.000, 214.000], loss: 34.642273, mean_absolute_error: 359.578644, mean_q: 392.649475\n",
            "  73698/1000000: episode: 31, duration: 116.243s, episode steps: 2377, steps per second: 20, episode reward: -20.000, mean reward: -0.008 [-2.000, 2.000], mean action: 8.616 [0.000, 17.000], mean observation: 124.851 [0.000, 214.000], loss: 35.033993, mean_absolute_error: 362.133698, mean_q: 395.266418\n",
            "  76072/1000000: episode: 32, duration: 116.461s, episode steps: 2374, steps per second: 20, episode reward: -32.000, mean reward: -0.013 [-2.000, 2.000], mean action: 8.166 [0.000, 17.000], mean observation: 124.875 [0.000, 214.000], loss: 34.449055, mean_absolute_error: 360.683685, mean_q: 393.054077\n",
            "  78460/1000000: episode: 33, duration: 116.664s, episode steps: 2388, steps per second: 20, episode reward: -22.000, mean reward: -0.009 [-2.000, 2.000], mean action: 8.222 [0.000, 17.000], mean observation: 124.846 [0.000, 214.000], loss: 33.138752, mean_absolute_error: 346.141907, mean_q: 377.380890\n",
            "  80822/1000000: episode: 34, duration: 115.957s, episode steps: 2362, steps per second: 20, episode reward: -19.000, mean reward: -0.008 [-2.000, 2.000], mean action: 8.163 [0.000, 17.000], mean observation: 124.850 [0.000, 214.000], loss: 33.115353, mean_absolute_error: 344.361145, mean_q: 375.358215\n",
            "  83224/1000000: episode: 35, duration: 117.816s, episode steps: 2402, steps per second: 20, episode reward: -6.000, mean reward: -0.002 [-2.000, 2.000], mean action: 9.366 [0.000, 17.000], mean observation: 124.887 [0.000, 214.000], loss: 32.160442, mean_absolute_error: 338.044952, mean_q: 368.252655\n",
            "  85588/1000000: episode: 36, duration: 115.781s, episode steps: 2364, steps per second: 20, episode reward: -15.000, mean reward: -0.006 [-2.000, 2.000], mean action: 7.527 [0.000, 17.000], mean observation: 124.872 [0.000, 214.000], loss: 33.208023, mean_absolute_error: 347.229492, mean_q: 378.304504\n",
            "  87954/1000000: episode: 37, duration: 117.103s, episode steps: 2366, steps per second: 20, episode reward: -24.000, mean reward: -0.010 [-2.000, 2.000], mean action: 7.921 [0.000, 17.000], mean observation: 124.842 [0.000, 214.000], loss: 33.078056, mean_absolute_error: 346.059540, mean_q: 376.675903\n",
            "  90333/1000000: episode: 38, duration: 117.115s, episode steps: 2379, steps per second: 20, episode reward: -12.000, mean reward: -0.005 [-2.000, 2.000], mean action: 7.995 [0.000, 17.000], mean observation: 124.851 [0.000, 214.000], loss: 30.660728, mean_absolute_error: 324.582275, mean_q: 352.819946\n",
            "  92733/1000000: episode: 39, duration: 118.641s, episode steps: 2400, steps per second: 20, episode reward: -16.000, mean reward: -0.007 [-2.000, 2.000], mean action: 8.717 [0.000, 17.000], mean observation: 124.840 [0.000, 214.000], loss: 31.259495, mean_absolute_error: 330.677826, mean_q: 359.933197\n",
            "  95124/1000000: episode: 40, duration: 118.220s, episode steps: 2391, steps per second: 20, episode reward: -12.000, mean reward: -0.005 [-2.000, 2.000], mean action: 8.349 [0.000, 17.000], mean observation: 124.858 [0.000, 214.000], loss: 30.159786, mean_absolute_error: 321.740417, mean_q: 350.058838\n",
            "  97490/1000000: episode: 41, duration: 116.857s, episode steps: 2366, steps per second: 20, episode reward: -24.000, mean reward: -0.010 [-2.000, 2.000], mean action: 7.965 [0.000, 17.000], mean observation: 124.841 [0.000, 214.000], loss: 30.599529, mean_absolute_error: 326.244049, mean_q: 354.919586\n",
            "  99871/1000000: episode: 42, duration: 118.129s, episode steps: 2381, steps per second: 20, episode reward: -20.000, mean reward: -0.008 [-2.000, 2.000], mean action: 7.943 [0.000, 17.000], mean observation: 124.843 [0.000, 214.000], loss: 32.384441, mean_absolute_error: 344.223999, mean_q: 374.380310\n",
            "Step 100002: saving model to dqn512_Boxing-v4_weights_100002.h5f\n",
            "Step 100002: saving model to Dropbox: dqn512_Boxing-v4_weights_100002.h5f\n",
            " 102237/1000000: episode: 43, duration: 120.821s, episode steps: 2366, steps per second: 20, episode reward: -10.000, mean reward: -0.004 [-2.000, 2.000], mean action: 9.151 [0.000, 17.000], mean observation: 124.882 [0.000, 214.000], loss: 31.934776, mean_absolute_error: 344.317566, mean_q: 374.920898\n",
            " 104582/1000000: episode: 44, duration: 116.869s, episode steps: 2345, steps per second: 20, episode reward: -21.000, mean reward: -0.009 [-2.000, 2.000], mean action: 7.458 [0.000, 17.000], mean observation: 124.838 [0.000, 214.000], loss: 32.584969, mean_absolute_error: 352.067871, mean_q: 382.879944\n",
            " 106969/1000000: episode: 45, duration: 119.109s, episode steps: 2387, steps per second: 20, episode reward: -7.000, mean reward: -0.003 [-2.000, 2.000], mean action: 9.103 [0.000, 17.000], mean observation: 124.861 [0.000, 214.000], loss: 33.981594, mean_absolute_error: 366.412018, mean_q: 398.355042\n",
            " 109373/1000000: episode: 46, duration: 119.761s, episode steps: 2404, steps per second: 20, episode reward: -33.000, mean reward: -0.014 [-2.000, 2.000], mean action: 7.990 [0.000, 17.000], mean observation: 124.846 [0.000, 214.000], loss: 32.025303, mean_absolute_error: 349.800629, mean_q: 379.812317\n",
            "done, took 5386.617 seconds\n",
            "Weights Saved:  dqn512_Boxing-v4_weights.h5f\n",
            "Testing for 10 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-adf977a8705e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Weights Saved: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/keras-rl/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/keras-rl/rl/agents/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# Select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/keras-rl/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/keras-rl/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "kak3CBgAWQYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}