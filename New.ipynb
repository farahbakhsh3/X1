{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hello, Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/farahbakhsh3/X1/blob/master/New.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "q3OYB0SGSl1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt install unrar\n",
        "!wget https://www.dropbox.com/s/rl6nx5e5h0ff4ub/lesson9.rar\n",
        "!unrar x -r lesson9.rar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_QaXjQ_ZvSue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "464415b7-81ee-4ec6-e336-2c45d66c7c32"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        " \n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        " \n",
        " \n",
        "# load data (raw text file)\n",
        "################################################################################\n",
        "################################################################################\n",
        " \n",
        "path = './lesson9/data/test.txt'\n",
        "with open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('corpus length:', len(text))\n",
        " \n",
        "# create two dictionaries: (index to id) & (id to index)\n",
        "################################################################################\n",
        "################################################################################\n",
        " \n",
        "chars = sorted(list(set(text)))\n",
        "len_of_char = len(chars)\n",
        "print('total chars:', len(chars))\n",
        "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
        "idx2char = dict((i, c) for i, c in enumerate(chars))\n",
        " \n",
        "# convert each character to an unique id\n",
        "################################################################################\n",
        "################################################################################\n",
        " \n",
        "max_length = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - max_length, step):\n",
        "    sentences.append(text[i: i + max_length])\n",
        "    next_chars.append(text[i + max_length])\n",
        "print('nb sequences:', len(sentences))\n",
        " \n",
        "# create proper vector for training model\n",
        "################################################################################\n",
        "################################################################################\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), max_length, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences)))\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char2idx[char]] = 1\n",
        "    y[i] = char2idx[next_chars[i]]\n",
        " \n",
        " \n",
        "# two helper functions to print generated text at the end of each epoch\n",
        "################################################################################\n",
        "################################################################################\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        " \n",
        "    start_index = random.randint(0, len(text) - max_length - 1)\n",
        "    for diversity in [1.0]:\n",
        "        print('----- diversity:', diversity)\n",
        " \n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + max_length]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        " \n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, max_length, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char2idx[char]] = 1.\n",
        " \n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            pred_id = sample(preds, diversity)\n",
        "            next_char = idx2char[pred_id]\n",
        " \n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        " \n",
        "        print(generated)\n",
        " \n",
        " \n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        " \n",
        " \n",
        "# Define simple GRU model for generating text\n",
        "# In case of using CPU replace the CuDNNGRU with GRU\n",
        "################################################################################\n",
        "################################################################################\n",
        " \n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.CuDNNGRU(256, input_shape=(max_length, len_of_char)))\n",
        "# model.add(keras.layers.GRU(256, input_shape=(max_length, len_of_char)))\n",
        "model.add(keras.layers.Dense(len_of_char, activation='softmax'))\n",
        " \n",
        "# Create graph and train the model for 60 epoch\n",
        "################################################################################\n",
        "################################################################################\n",
        " \n",
        "model.compile(optimizer=keras.optimizers.RMSprop(0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "print(model.summary())\n",
        "print_callback = keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "model.fit(x=x, y=y, epochs=60, batch_size=128, callbacks=[print_callback])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0ca6e6031449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../lesson9/data/test.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpus length:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../lesson9/data/test.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Oicrf_z7v5hi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}